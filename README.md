# Adversarial-Attacks-on-Machine-Learning-Models
The machine learning model can be easily fooled to incorrectly classify an input sample which was structurally and intentionally modi ed. These perturbed samples created from the original data set by making the worst case changes are called adversarial examples, and this act of fooling the model is called adversarial attacks. This vulnerability of the machine learning models to force misclassification is a major security concern since such models are deployed at various locations for various tasks.
